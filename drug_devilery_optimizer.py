# -*- coding: utf-8 -*-
"""Drug Devilery Optimizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JBiWaEe-m37z2rGTuAn-oZtD5rTZw3zr
"""



# 確保這行在 Colab 執行！
!pip install -q tensorflow

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.multioutput import MultiOutputRegressor
from sklearn.linear_model import Ridge
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler

# 載入資料
X = pd.read_csv('X_features.csv')
y = pd.read_csv('y_targets.csv')
X = X.dropna(); y = y.loc[X.index]
y = y.dropna(); X = X.loc[y.index]

# 標準化處理
X_scaler = StandardScaler()
X_scaled = X_scaler.fit_transform(X)
y_scaler = StandardScaler()
y_scaled = y_scaler.fit_transform(y)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.15, random_state=42)

histories = {'MLP': {'loss': [], 'val_loss': [], 'r2': []}}

mlp_model = Sequential([
    Input(shape=(X.shape[1],)),
    Dense(128, activation='relu'),
    Dense(128, activation='relu'),
    Dense(y.shape[1])
])
mlp_model.compile(optimizer=Adam(1e-4), loss='mse')

class MLPHistoryCallback(Callback):
    def on_epoch_end(self, epoch, logs=None):
        histories['MLP']['loss'].append(logs['loss'])
        histories['MLP']['val_loss'].append(logs['val_loss'])
        y_pred = self.model.predict(X_test, verbose=0)
        y_pred_inv = y_scaler.inverse_transform(y_pred)
        y_test_inv = y_scaler.inverse_transform(y_test)
        r2 = r2_score(y_test_inv, y_pred_inv)
        histories['MLP']['r2'].append(r2)
        print(f"Epoch {epoch+1:03d} - Loss: {logs['loss']:.2e} - Val Loss: {logs['val_loss']:.2e} - R²: {r2:.4f}")

# 學習率調整函數（每300個epoch將lr減少為原來的0.1倍）
def scheduler(epoch, lr):
    if epoch > 0 and epoch % 300 == 0:
        return lr * 0.1
    return lr

mlp_model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=1000,
    batch_size=64,
    verbose=0,
    callbacks=[
        MLPHistoryCallback(),
        EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True),
        LearningRateScheduler(scheduler)
    ]
)
# 用未標準化的資料訓練 Ridge 和 GBDT
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X, y, test_size=0.15, random_state=42)
ridge_model = MultiOutputRegressor(Ridge(alpha=1.0)).fit(X_train_orig, y_train_orig)
ridge_r2 = r2_score(y_test_orig, ridge_model.predict(X_test_orig))

gbdt_model = MultiOutputRegressor(GradientBoostingRegressor(n_estimators=100)).fit(X_train_orig, y_train_orig)
gbdt_r2 = r2_score(y_test_orig, gbdt_model.predict(X_test_orig))

# 可視化 loss 和 R²
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(histories['MLP']['loss'], label='Training Loss')
plt.plot(histories['MLP']['val_loss'], label='Validation Loss')
plt.xlabel('Epoch'); plt.ylabel('MSE Loss'); plt.title('MLP Training & Validation Loss'); plt.legend()

plt.subplot(1, 2, 2)
plt.plot(histories['MLP']['r2'], label='MLP R²')
plt.xlabel('Epoch'); plt.ylabel('R² Score'); plt.title('MLP R² Score over Epochs'); plt.ylim(0, 1); plt.legend()
plt.tight_layout(); plt.show()

# 最終 R² 條狀圖
final_mlp_r2 = histories['MLP']['r2'][-1] if histories['MLP']['r2'] else 0
plt.figure(figsize=(6, 4))
bars = plt.bar(['MLP', 'Ridge', 'GBDT'], [final_mlp_r2, ridge_r2, gbdt_r2],
               color=['skyblue', 'lightgreen', 'salmon'])
for bar, r2 in zip(bars, [final_mlp_r2, ridge_r2, gbdt_r2]):
    plt.text(bar.get_x() + bar.get_width() / 2, min(bar.get_height() + 0.02, 0.98), f"{r2:.3f}", ha='center')
plt.ylabel('R² Score'); plt.title('Final R² Comparison of Models'); plt.ylim(0, 1); plt.show()

import matplotlib.pyplot as plt

# 直接键入各模型的平均 R²
model_names = ['TabNet', 'MLP', 'DCN', 'XGBoost', 'GBDT']
r2_values   = [0.421,   0.489,  0.549,  0.392,     0.273]

# 绘图
plt.figure(figsize=(8, 5))
bars = plt.bar(model_names, r2_values,
               color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'],
               edgecolor='black')

plt.ylim(0, 1)
plt.ylabel('Average R²')
plt.title('Model Comparison: Average R²')

# 在每个柱子上方标注数值
for bar, r2 in zip(bars, r2_values):
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.02,
             f'{r2:.3f}', ha='center', va='bottom', fontsize=12)

plt.grid(axis='y', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

# 如果在 Colab 里还没装好这些库，请先运行：
# !pip install pytorch-tabnet xgboost --quiet

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection   import train_test_split
from sklearn.preprocessing     import StandardScaler
from sklearn.metrics           import r2_score
from sklearn.multioutput       import MultiOutputRegressor
from xgboost                   import XGBRegressor

import tensorflow as tf
from tensorflow.keras          import Input, Model
from tensorflow.keras.layers   import Dense, Dropout, concatenate, Layer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks  import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

# ---------------------------------------------------------------------------- #
# 1) 载入 & 预处理
# ---------------------------------------------------------------------------- #
X = pd.read_csv('X_features.csv').dropna()
y = pd.read_csv('y_targets.csv').loc[X.index]
# 对 targets 做 log1p 稳定训练
y_log = np.log1p(y.values)

# 划分
X_train, X_test, y_train_log, y_test_log = train_test_split(
    X.values, y_log, test_size=0.15, random_state=42
)

# 只对 features 标准化
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_test  = scaler.transform(X_test)

# ---------------------------------------------------------------------------- #
# 2) 实时 R² 打印 Callback（Keras专用）
# ---------------------------------------------------------------------------- #
class R2Logger(Callback):
    def __init__(self, X_val, y_val_log, name="Model"):
        super().__init__()
        self.X_val     = X_val
        self.y_val_log = y_val_log
        self.name      = name

    def on_epoch_end(self, epoch, logs=None):
        preds_log = self.model.predict(self.X_val, verbose=0)
        y_true    = np.expm1(self.y_val_log)
        y_pred    = np.expm1(preds_log)
        r2        = r2_score(y_true, y_pred)
        print(f"{self.name} ▶ epoch {epoch+1:03d} "
              f"- loss {logs['loss']:.4e} "
              f"- val_loss {logs['val_loss']:.4e} "
              f"- val_R² {r2:.4f}")

# ---------------------------------------------------------------------------- #
# 3) Base Learner #1: MLP
# ---------------------------------------------------------------------------- #
def build_mlp(input_dim, output_dim):
    inp = Input(shape=(input_dim,))
    x   = Dense(128, activation='relu')(inp)
    x   = Dense(128, activation='relu')(x)
    out = Dense(output_dim, activation=None)(x)
    return Model(inp, out)

mlp = build_mlp(X_train.shape[1], y_train_log.shape[1])
mlp.compile(optimizer=Adam(1e-4), loss='mse')

mlp_cbs = [
    R2Logger(X_test, y_test_log, name="MLP"),
    EarlyStopping('val_loss', patience=20, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau('val_loss', factor=0.5, patience=10, verbose=1),
    ModelCheckpoint('best_mlp.h5', monitor='val_loss', save_best_only=True, verbose=0)
]

print("\n▶▶▶ 开始训练 MLP …")
mlp.fit(
    X_train, y_train_log,
    validation_data=(X_test, y_test_log),
    epochs=500,
    batch_size=64,
    callbacks=mlp_cbs,
    verbose=0
)
mlp.load_weights('best_mlp.h5')
pred_mlp_log = mlp.predict(X_test, verbose=0)
pred_mlp     = np.expm1(pred_mlp_log)

# ---------------------------------------------------------------------------- #
# 4) Base Learner #2: DCN
# ---------------------------------------------------------------------------- #
class CrossLayer(Layer):
    def build(self, input_shape):
        dim = input_shape[0][-1]
        self.w = self.add_weight(name="w", shape=(dim,1),
                                 initializer="random_normal", trainable=True)
        self.b = self.add_weight(name="b", shape=(dim,),
                                 initializer="zeros", trainable=True)
    def call(self, inputs):
        x0, xl = inputs
        dot = tf.matmul(xl, self.w)
        return x0 * dot + self.b + xl

def build_dcn(input_dim, cross_layers=3, deep_units=[256,128,64], dropout_rate=0.3):
    x0 = Input(shape=(input_dim,), name="features")
    xl = x0
    for _ in range(cross_layers):
        xl = CrossLayer()([x0, xl])
    xd = x0
    for units in deep_units:
        xd = Dense(units, activation="relu")(xd)
        xd = Dropout(dropout_rate)(xd)
    x = concatenate([xl, xd])
    out = Dense(y_train_log.shape[1], activation=None, name="output")(x)
    return Model(inputs=x0, outputs=out)

dcn = build_dcn(X_train.shape[1])
dcn.compile(optimizer=Adam(1e-3), loss='mse')

dcn_cbs = [
    R2Logger(X_test, y_test_log, name="DCN"),
    EarlyStopping('val_loss', patience=20, restore_best_weights=True, verbose=1),
    ReduceLROnPlateau('val_loss', factor=0.5, patience=10, verbose=1),
    ModelCheckpoint('best_dcn.h5', monitor='val_loss', save_best_only=True, verbose=0)
]

print("\n▶▶▶ 开始训练 DCN …")
dcn.fit(
    X_train, y_train_log,
    validation_data=(X_test, y_test_log),
    epochs=500,
    batch_size=256,
    callbacks=dcn_cbs,
    verbose=0
)
dcn.load_weights('best_dcn.h5')
pred_dcn_log = dcn.predict(X_test, verbose=0)
pred_dcn     = np.expm1(pred_dcn_log)

# ---------------------------------------------------------------------------- #
# 5) Base Learner #3: XGBoost
# ---------------------------------------------------------------------------- #
print("\n▶▶▶ 开始训练 XGBoost …")
xgb_base = XGBRegressor(
    objective='reg:squarederror',
    n_estimators=200,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    n_jobs=-1,
    verbosity=1,
    eval_metric='rmse'
)
xgb_model = MultiOutputRegressor(xgb_base, n_jobs=-1)
xgb_model.fit(X_train, y_train_log)

pred_xgb_log = xgb_model.predict(X_test)
pred_xgb     = np.expm1(pred_xgb_log)

# ---------------------------------------------------------------------------- #
# 6) 简单平均 Ensemble & 最终评估
# ---------------------------------------------------------------------------- #
pred_ens = (pred_mlp + pred_dcn + pred_xgb) / 3.0

r2_mlp = r2_score(np.expm1(y_test_log), pred_mlp)
r2_dcn = r2_score(np.expm1(y_test_log), pred_dcn)
r2_xgb = r2_score(np.expm1(y_test_log), pred_xgb)
r2_ens = r2_score(np.expm1(y_test_log), pred_ens)

print("\n--- 单模型 R² 分别：")
print(f"MLP      = {r2_mlp:.4f}")
print(f"DCN      = {r2_dcn:.4f}")
print(f"XGBoost  = {r2_xgb:.4f}")
print(f"Ensemble = {r2_ens:.4f}")

# ---------------------------------------------------------------------------- #
# 7) 可视化示例：第一维输出（u_max）True vs Pred
# ---------------------------------------------------------------------------- #
plt.figure(figsize=(6,6))
true = np.expm1(y_test_log)[:,0]
plt.scatter(true, pred_mlp[:,0], label='MLP',   alpha=0.5)
plt.scatter(true, pred_dcn[:,0], label='DCN',   alpha=0.5)
plt.scatter(true, pred_xgb[:,0], label='XGBoost', alpha=0.5)
plt.scatter(true, pred_ens[:,0], label='Ensemble', c='k', s=10)
mn, mx = true.min(), true.max()
plt.plot([mn,mx],[mn,mx],'r--')
plt.xlabel("True u_max"); plt.ylabel("Predicted u_max")
plt.legend(); plt.title("u_max: True vs Predicted")
plt.tight_layout()
plt.show()

# 如果在 Colab/GPU 上，请先安装依赖：
# !pip install xgboost tensorflow scikit-learn --quiet

import numpy as np
import pandas as pd
from sklearn.model_selection   import train_test_split
from sklearn.preprocessing     import StandardScaler
from sklearn.metrics           import r2_score
from sklearn.multioutput       import MultiOutputRegressor
from xgboost                   import XGBRegressor
import tensorflow as tf
from tensorflow.keras          import Input, Model
from tensorflow.keras.layers   import Dense, Dropout, concatenate, Layer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks  import Callback, EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt

# ─── 1) 载入 & 预处理 ───────────────────────────────────────────────
X_df = pd.read_csv("X_features.csv").dropna()
y_df = pd.read_csv("y_targets.csv").loc[X_df.index]

X = X_df.values
y_log = np.log1p(y_df.values)   # 对 targets 做 log1p

# 划分训练/验证
X_train, X_val, y_train_log, y_val_log = train_test_split(
    X, y_log, test_size=0.15, random_state=42
)

# 标准化 features
scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_val   = scaler.transform(X_val)


# ─── 2) 定义 DCN 基学习器 ───────────────────────────────────────────
class CrossLayer(Layer):
    def build(self, input_shape):
        dim = input_shape[0][-1]
        # 注意必须用 name=, 否则 Keras 识别 pos arg 为 shape
        self.w = self.add_weight(name="w", shape=(dim,1),
                                 initializer="random_normal", trainable=True)
        self.b = self.add_weight(name="b", shape=(dim,),
                                 initializer="zeros", trainable=True)
    def call(self, inputs):
        x0, xl = inputs
        dot = tf.matmul(xl, self.w)       # (batch,1)
        return x0 * dot + self.b + xl

def build_dcn(input_dim, output_dim,
              cross_layers=3, deep_units=[256,128,64], dropout_rate=0.3):
    x0 = Input(shape=(input_dim,), name="features")
    # Cross network
    xl = x0
    for _ in range(cross_layers):
        xl = CrossLayer()([x0, xl])
    # Deep network
    xd = x0
    for units in deep_units:
        xd = Dense(units, activation="relu")(xd)
        xd = Dropout(dropout_rate)(xd)
    # 合并 & 输出
    merged = concatenate([xl, xd])
    out = Dense(output_dim, activation=None, name="output")(merged)
    return Model(x0, out)

# R² 实时打印回调
class R2Logger(Callback):
    def __init__(self, X_val, y_val_log, name="Model"):
        super().__init__()
        self.X_val, self.y_val_log, self.name = X_val, y_val_log, name
    def on_epoch_end(self, epoch, logs=None):
        pred_log = self.model.predict(self.X_val, verbose=0)
        y_true   = np.expm1(self.y_val_log)
        y_pred   = np.expm1(pred_log)
        r2 = r2_score(y_true, y_pred)
        print(f"{self.name} ▶ epoch {epoch+1:03d} "
              f"- loss {logs['loss']:.4e} "
              f"- val_loss {logs['val_loss']:.4e} "
              f"- val_R² {r2:.4f}")

# 实例化 & 训练 DCN
tf.random.set_seed(42)
dcn = build_dcn(X_train.shape[1], y_train_log.shape[1])
dcn.compile(optimizer=Adam(1e-3), loss="mse")

print("\n▶▶▶ 训练 DCN …")
dcn.fit(
    X_train, y_train_log,
    validation_data=(X_val, y_val_log),
    epochs=200,
    batch_size=256,
    callbacks=[
        R2Logger(X_val, y_val_log, name="DCN"),
        EarlyStopping("val_loss", patience=20, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau("val_loss", factor=0.5, patience=10, verbose=1)
    ],
    verbose=0
)

pred_train_dcn = dcn.predict(X_train)
pred_val_dcn   = dcn.predict(X_val)


# ─── 3) 定义 XGBoost 基学习器 ───────────────────────────────────────
print("\n▶▶▶ 训练 XGBoost …")
xgb_base = XGBRegressor(
    objective='reg:squarederror',
    n_estimators=500,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    n_jobs=-1
)
xgb = MultiOutputRegressor(xgb_base)
xgb.fit(X_train, y_train_log)

pred_train_xgb = xgb.predict(X_train)
pred_val_xgb   = xgb.predict(X_val)


# ─── 4) 构造元特征 & 训练 Meta Learner ───────────────────────────────
meta_train = np.hstack([pred_train_dcn, pred_train_xgb])  # (N_train, 12)
meta_val   = np.hstack([pred_val_dcn,   pred_val_xgb])    # (N_val,   12)

inp = Input(shape=(meta_train.shape[1],))
h = Dense(64, activation="relu")(inp)
h = Dense(32, activation="relu")(h)
out = Dense(y_train_log.shape[1], activation=None)(h)
meta_mlp = Model(inp, out)
meta_mlp.compile(optimizer=Adam(1e-3), loss="mse")

print("\n▶▶▶ 训练 Meta Learner …")
meta_mlp.fit(
    meta_train, y_train_log,
    validation_data=(meta_val, y_val_log),
    epochs=500,
    batch_size=64,
    callbacks=[
        R2Logger(meta_val, y_val_log, name="Meta-MLP"),
        EarlyStopping("val_loss", patience=20, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau("val_loss", factor=0.5, patience=10, verbose=1)
    ],
    verbose=0
)

# ─── 5) 最终评估 ───────────────────────────────────────────────────
pred_meta_log = meta_mlp.predict(meta_val)
pred_meta     = np.expm1(pred_meta_log)
true          = np.expm1(y_val_log)

r2_scores = [r2_score(true[:,i], pred_meta[:,i]) for i in range(true.shape[1])]
print("\n▶▶▶ Stacking Ensemble 各输出 R²：")
for idx, val in enumerate(r2_scores, start=1):
    print(f"  target{idx}: R² = {val:.4f}")
print("  平均 R² =", np.mean(r2_scores))

# 如果在 Colab/GPU 上，请先安装依赖：
# !pip install xgboost lightgbm tensorflow scikit-learn --quiet

import numpy as np
import pandas as pd
from sklearn.model_selection   import train_test_split
from sklearn.preprocessing     import StandardScaler
from sklearn.metrics           import r2_score
from sklearn.multioutput       import MultiOutputRegressor

from xgboost                   import XGBRegressor
from lightgbm                  import LGBMRegressor

import tensorflow as tf
from tensorflow.keras          import Input, Model
from tensorflow.keras.layers   import Dense, Dropout, concatenate, Layer, LayerNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks  import Callback, EarlyStopping, ReduceLROnPlateau

import matplotlib.pyplot as plt

# ─── 1) 载入 & 预处理 ───────────────────────────────────────────────
X_df = pd.read_csv("X_features.csv").dropna()
y_df = pd.read_csv("y_targets.csv").loc[X_df.index]

X = X_df.values
y_log = np.log1p(y_df.values)   # 对 targets 做 log1p

X_train, X_val, y_train_log, y_val_log = train_test_split(
    X, y_log, test_size=0.15, random_state=42
)

scaler = StandardScaler().fit(X_train)
X_train = scaler.transform(X_train)
X_val   = scaler.transform(X_val)


# ─── 2) DCN 基学习器 定义 + 训练 ───────────────────────────────────


print("\n▶▶▶ 训练 XGBoost …")
xgb_models = []
for i in range(y_train_log.shape[1]):
    print(f"▶ 訓練 target {i+1}")
    model = XGBRegressor(
        objective='reg:squarederror',
        n_estimators=300,  # 保守設定，避免過擬合
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        eval_metric="rmse",  # 手動指定評估指標
        random_state=42,
        n_jobs=-1
    )
    model.fit(
        X_train, y_train_log[:, i],
        eval_set=[(X_val, y_val_log[:, i])],
        verbose=100
    )
    xgb_models.append(model)

# 取得預測結果
pred_train_xgb = np.column_stack([model.predict(X_train) for model in xgb_models])
pred_val_xgb   = np.column_stack([model.predict(X_val) for model in xgb_models])





# ─── 2) DCN 基学习器 定义 + 训练 ───────────────────────────────────
class CrossLayer(Layer):
    def build(self, input_shape):
        dim = input_shape[0][-1]
        # name= 必须指定，否则 add_weight 会误把第一个参数当 shape
        self.w = self.add_weight(name="w", shape=(dim,1),
                                 initializer="random_normal", trainable=True)
        self.b = self.add_weight(name="b", shape=(dim,),
                                 initializer="zeros", trainable=True)
        self.ln = LayerNormalization()  # 加入 LayerNorm
    def call(self, inputs):
        x0, xl = inputs
        dot = tf.matmul(xl, self.w)       # (batch,1)
        x_cross = x0 * dot + self.b + xl
        return self.ln(x_cross)           # 输出归一化后的特征交叉

def build_dcn(input_dim, output_dim,
              cross_layers=3, deep_units=[256,128,64], dropout_rate=0.3):
    x0 = Input(shape=(input_dim,), name="features")
    xl = x0
    for _ in range(cross_layers):
        xl = CrossLayer()([x0, xl])
    xd = x0
    for units in deep_units:
        xd = Dense(units, activation="relu")(xd)
        xd = Dropout(dropout_rate)(xd)
    merged = concatenate([xl, xd])
    out = Dense(output_dim, activation=None, name="output")(merged)
    return Model(x0, out)

class R2Logger(Callback):
    def __init__(self, X_val, y_val_log, name="Model"):
        super().__init__()
        self.X_val, self.y_val_log, self.name = X_val, y_val_log, name
    def on_epoch_end(self, epoch, logs=None):
        pred_log = self.model.predict(self.X_val, verbose=0)
        y_true   = np.expm1(self.y_val_log)
        y_pred   = np.expm1(pred_log)
        r2 = r2_score(y_true, y_pred)
        print(f"{self.name} ▶ epoch {epoch+1:03d} "
              f"- loss {logs['loss']:.4e} "
              f"- val_loss {logs['val_loss']:.4e} "
              f"- val_R² {r2:.4f}")

# 训练 DCN
tf.random.set_seed(42)
dcn = build_dcn(X_train.shape[1], y_train_log.shape[1])
dcn.compile(optimizer=Adam(1e-3), loss="mse")

print("\n▶▶▶ 训练 DCN …")
dcn.fit(
    X_train, y_train_log,
    validation_data=(X_val, y_val_log),
    epochs=400,
    batch_size=256,
    callbacks=[
        R2Logger(X_val, y_val_log, name="DCN"),
        EarlyStopping("val_loss", patience=20, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau("val_loss", factor=0.5, patience=10, verbose=1)
    ],
    verbose=0
)

pred_train_dcn = dcn.predict(X_train)
pred_val_dcn   = dcn.predict(X_val)








# ─── 4) Residual Boosting + LightGBM Meta Learner ───────────────────
meta_init_train = 0.5 * (pred_train_dcn + pred_train_xgb)
meta_init_val   = 0.5 * (pred_val_dcn   + pred_val_xgb)

resid_train = y_train_log - meta_init_train

meta_features_train = np.hstack([pred_train_dcn, pred_train_xgb])
meta_features_val   = np.hstack([pred_val_dcn,   pred_val_xgb])

res_base = LGBMRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=4,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1
)
res_model = MultiOutputRegressor(res_base)
res_model.fit(meta_features_train, resid_train)

resid_pred_val = res_model.predict(meta_features_val)
final_pred_log = meta_init_val + resid_pred_val
final_pred     = np.expm1(final_pred_log)
true_val       = np.expm1(y_val_log)

# ─── 5) 最终评估 ───────────────────────────────────────────────────
r2_scores = [r2_score(true_val[:,i], final_pred[:,i]) for i in range(true_val.shape[1])]
print("\n▶▶▶ 最终 Stacking＋Residual Boosting 各输出 R²：")
for idx, val in enumerate(r2_scores, start=1):
    print(f"  target{idx}: R² = {val:.4f}")
print("  平均 R² =", np.mean(r2_scores))

# ─── 6) 可视化第一维输出（u_max）True vs Pred ───────────────────────
plt.figure(figsize=(6,6))
mn, mx = true_val[:,0].min(), true_val[:,0].max()
plt.scatter(true_val[:,0], final_pred[:,0], alpha=0.4)
plt.plot([mn, mx], [mn, mx], 'r--')
plt.xlabel("True u_max"); plt.ylabel("Pred u_max")
plt.title(f"u_max: True vs Pred (R²={r2_scores[0]:.3f})")
plt.show()

from tensorflow.keras.layers import Input, Dense, Dropout, concatenate, Layer, LayerNormalization
from tensorflow.keras.models import Model
import tensorflow as tf
from tensorflow.keras.layers import Activation
# 優化版 CrossLayer：模仿 DCN-V2，使用 low-rank matrix kernel
class CrossLayerV2(Layer):
    def __init__(self, rank=8):
        super().__init__()
        self.rank = rank

    def build(self, input_shape):
        dim = input_shape[0][-1]
        self.W = self.add_weight(name="W", shape=(dim, self.rank),
                                 initializer="glorot_uniform", trainable=True)
        self.v = self.add_weight(name="v", shape=(self.rank, 1),
                                 initializer="glorot_uniform", trainable=True)
        self.b = self.add_weight(name="b", shape=(dim,),
                                 initializer="zeros", trainable=True)
        self.ln = LayerNormalization()

    def call(self, inputs):
        x0, xl = inputs
        x_proj = tf.matmul(xl, self.W)      # (batch, rank)
        x_proj = tf.matmul(x_proj, self.v)  # (batch, 1)
        x_proj = tf.multiply(x_proj, x0)    # (batch, dim)
        out = x_proj + self.b + xl
        return self.ln(out)

def build_dcn(input_dim, output_dim,
              cross_layers=5, cross_rank=8,
              deep_units=[128, 64], dropout_rate=0.2):
    x0 = Input(shape=(input_dim,), name="features")

    # Cross 分支
    xl = x0
    for _ in range(cross_layers):
        xl = CrossLayerV2(rank=cross_rank)([x0, xl])

    # Deep 分支
    xd = x0
    for units in deep_units:
        xd = Dense(units)(xd)
        xd = LayerNormalization()(xd)
        xd = Activation("swish")(xd)  # ✅ 改為合法 Keras 操作
        xd = Dropout(dropout_rate)(xd)

    # 合併並輸出
    merged = concatenate([xl, xd])
    out = Dense(output_dim, activation=None, name="output")(merged)
    return Model(x0, out)

tf.random.set_seed(42)
dcn = build_dcn(X_train.shape[1], y_train_log.shape[1])
dcn.compile(optimizer=Adam(1e-3), loss="mse")

print("\n▶▶▶ 训练 DCN …")
dcn.fit(
    X_train, y_train_log,
    validation_data=(X_val, y_val_log),
    epochs=250,
    batch_size=256,
    callbacks=[
        R2Logger(X_val, y_val_log, name="DCN"),
        EarlyStopping("val_loss", patience=20, restore_best_weights=True, verbose=1),
        ReduceLROnPlateau("val_loss", factor=0.5, patience=10, verbose=1)
    ],
    verbose=0
)

pred_train_dcn = dcn.predict(X_train)
pred_val_dcn   = dcn.predict(X_val)

